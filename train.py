import argparseimport numpy as npimport torchimport torch.optim as optimimport torch.nn as nnimport torch.nn.functional as Fimport torchvisionimport matplotlib.pyplot as pltfrom model.mlp import Generator, Discriminatorfrom model.cnn import Generator_CNN, Discriminator_CNNfrom utils.loss_gan import loss_G, loss_Dfrom utils.loss_wgan import loss_wG, loss_wDfrom utils.data import train_loader# argparse -----------------------------------------------------parser = argparse.ArgumentParser(description='Model and Parameters for GAN Training')parser.add_argument('--model', type=str, default='CNN', help='wheter model you use')parser.add_argument('--loss', type=str, default='gan', help='wheter loss you use')parser.add_argument('--clip_value', type=float, default=1e-2)parser.add_argument('--use_sig', action='store_false')parser.add_argument('--epoch', type=int, default='100')parser.add_argument('--batch_size', type=int, default='32')parser.add_argument('--train_g', type=int, default='1')parser.add_argument('--train_d', type=int, default='1', help='set 5, if you use wgan loss')args = parser.parse_args()# cuda ---------------------------------------------------------device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')# clip_grad_value ----------------------------------------------def clip_grad_value_(parameters, clip_value):    clip_value = float(clip_value)    for p in filter(lambda p: p.grad is not None, parameters):        p.grad.data.clamp_(min=-clip_value, max=clip_value)# nan error check ----------------------------------------------def nan_inf_error(input_, N):    # loss check    flag = False    if torch.isnan(input_) == 1:        flag = True        print('-----{} loss is nan-----'.format(N))    elif torch.max(input_).item() == float('Inf'):        flag = True        print('-----{} loss is if-----'.format(N))    if flag:        print('i: ', i)        print('z: ', z)        if N == 'D':            print('output_r: ', output_r)        print('output_f: ', output_f)        print('loss: ', loss)    return flag# train ---------------------------------------------------------batch_size = args.batch_sizeepochs = args.epochtrain_loader = train_loader(args.batch_size)cycle = torch.floor(torch.FloatTensor([len(train_loader)], device=device) / 10).item()if args.model == 'CNN':    G = Generator_CNN()    D = Discriminator_CNN()else:    G = Generator()    D = Discriminator()G.to(device)D.to(device)if args.loss == 'gan':    optimizer_G = optim.Adam(G.parameters(), lr=1e-3)    optimizer_D = optim.SGD(D.parameters(), lr=1e-3, momentum=0.9)if args.loss == 'wgan':    optimizer_G = optim.RMSprop(G.parameters(), lr=5e-5)    optimizer_D = optim.RMSprop(D.parameters(), lr=5e-5)flag = Falsefor epoch in range(epochs):    running_loss_G = 0.0    running_loss_D = 0.0    for i, data in enumerate(train_loader, 0):        # noise        if args.model == 'CNN':            z = torch.randn(batch_size, 100, device=device).view(-1, 100, 1, 1)        else:            z = torch.rand(batch_size, 1, device=device)        # opt D        D_loss = 0        for j in range(args.train_d):            optimizer_D.zero_grad()            # real            real = data[0].view(-1, 1, 28, 28)            real = real.to(device)            output_r = D(real, args.use_sig)            # fake            fake = G(z)            output_f = D(fake, args.use_sig)            if args.loss == 'gan':                loss = loss_D(output_r, output_f, batch_size)            elif args.loss == 'wgan':                loss = loss_wD(output_r, output_f, batch_size)            # loss check            flag = nan_inf_error(loss, 'D')            if flag:                break            loss.backward()            optimizer_D.step()            clip_grad_value_(D.parameters(), args.clip_value)            D_loss += loss.item()        if flag:            break        G_loss = 0        for j in range(args.train_g):            # opt G            optimizer_G.zero_grad()            fake = G(z)            output_f = D(fake)            if args.loss == 'gan':                loss = loss_G(output_f, batch_size)            elif args.loss == 'wgan':                loss = loss_wG(output_f, batch_size)            # loss check            flag = nan_inf_error(loss, 'G')            if flag:                break            loss.backward()            optimizer_G.step()            G_loss += loss.item()        if flag:            break        running_loss_G += G_loss        running_loss_D += D_loss        if i % cycle == 0:            print('[%d, %6d] loss_G: %f loss_D: %f' %                    (epoch, i, running_loss_G / (cycle * batch_size), running_loss_D / (cycle * batch_size)))            running_loss_G = 0.0            running_loss_D = 0.0    if flag:        break    # model save    torch.save(G.state_dict(), './weights/G.pth')    torch.save(D.state_dict(), './weights/D.pth')